---
title: "Análise dos dados de Consolidação Final"
author: 
- name: "Fellipe Gomes"
  affliliation: Universidade Federal Fluminense
date: "25 de outubro de 2017"
output: 
  html_document:
    toc: true
    toc_float: true
geometry: margin=lin


---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#Pacotes utilizados:
library(dplyr)
library(R2OpenBUGS)
library(ggplot2)
library(GGally)
library(coda)
library(lattice)
library(rpart)
library(rpart.plot)
library(gridExtra)
library(ROCR)
library(tabplot)
library(knitr)
library(kableExtra)
library(lubridate)
library(RSNNS)
library(caret)
library(FSelector)   #Metodos para selecionar os atributos
library(RSNNS)
library(nnet)
library(devtools)
library(scales)
library(reshape)
library(rattle)

#Dados que serão utilizados:
base=readRDS("modelo.rds")
basefator=readRDS("modelofator.rds")
dim(base)
dim(basefator)

dados=na.omit(base)
dados=dados[,-c(4,5,6,12,13,16)]
glimpse(dados)
glimpse(basefator)
basefator$condenado=as.factor(basefator$condenado)
basefator$condenado=relevel(basefator$condenado, "Sim")
```

#### Realizou-se a seleção das seguintes variáveis:

```{r,echo=F}
kable(basefator, "html")%>%
  kable_styling()%>%
  scroll_box(width = "1000px", height = "500px")
```

####De maneira visual:


```{r, echo=F}
tableplot(basefator, sortCol = condenado)
```

#### Observando as correlações:

```{r,echo=F}
g1=ggcorr(dados[,],method = c("pairwise", "spearman"), nbreaks = 9)
g2=ggcorr(dados[,],method = c("pairwise", "spearman"), nbreaks = 12)
gridExtra::grid.arrange(g1,g2,ncol=2)
```

#### Gráfico de Dispersão utilizando modelo logístico

Além disso, podemos visualizar os gráficos de dispersão com ajustes de regressões logísticas de cada variável para explicar a probabilidade de ser condenado, veja:

```{r, echo=F}
#Funcao de correlacoes
my_fn <- function(data, mapping, method="glm", ...){
  p <- ggplot(data = data, mapping = mapping) + 
    geom_point() + 
    geom_smooth(method=method, ...)
  p
}

ggpairs(dados, lower = list(continuous = my_fn))

```

#### Portando, vejamos mais detalhadamente essas variáveis

### Estatística descritiva

```{r,echo=F}
na.omit(basefator[,-2])%>%
summary()%>%
  t()%>%
  kable( "html")%>%
  kable_styling()

```

Visualmente:

```{r,echo=F, warning=F}


g1=ggplot() + geom_bar(aes(x = juiz, fill = juiz),position = 'dodge', data = na.omit(basefator), stat="count")

g2=ggplot() + geom_bar(aes(x = sexo, fill = sexo),position = 'dodge', data = na.omit(basefator), stat="count")

g3=ggplot() + geom_bar(aes(x = quantidade_de_reus, fill = quantidade_de_reus),position = 'dodge', data = na.omit(basefator), stat="count")

g4=ggplot() + geom_bar(aes(x = reincidente, fill = reincidente),position = 'dodge', data = na.omit(basefator), stat="count")

g5=ggplot() + geom_bar(aes(x = favela, fill = favela),position = 'dodge', data = na.omit(basefator), stat="count")

g6=ggplot() + geom_bar(aes(x = testemunha_def, fill = testemunha_def),position = 'dodge', data = na.omit(basefator), stat="count")

g7=ggplot() + geom_bar(aes(x = testemunha_acus, fill = testemunha_acus),position = 'dodge', data = na.omit(basefator), stat="count")

g8=ggplot() + geom_bar(aes(x = flagrante, fill = flagrante),position = 'dodge', data = na.omit(basefator), stat="count")

g9=ggplot() + geom_bar(aes(x = muita_quantidade, fill = muita_quantidade),position = 'dodge', data = na.omit(basefator), stat="count")

g10=ggplot() + geom_bar(aes(x = condicao_pessoal, fill = condicao_pessoal),position = 'dodge', data = na.omit(basefator), stat="count")

g11=ggplot() + geom_bar(aes(x = sumula, fill = sumula),position = 'dodge', data = na.omit(basefator), stat="count")

g12=ggplot() + geom_bar(aes(x = confissao, fill = confissao),position = 'dodge', data = na.omit(basefator), stat="count")

gridExtra::grid.arrange(g1,g2,g3,g4,g5,g6,g7,g8,g9,g10,g11,g12,ncol=2)

```

#### Para os que foram condenados:

```{r, echo=F}
a=by(basefator[,-2],basefator$condenado, summary)

a$Sim%>%
  t()%>%
  kable( "html")%>%
  kable_styling()
```

#### Para os que nao foram condenados:

```{r, echo=F}
a=by(basefator[,-2],basefator$condenado, summary)

a$Nao%>%
  t()%>%
  kable( "html")%>%
  kable_styling()
```
Visualmente:

```{r,echo=F, warning=F}


g1=ggplot() + geom_bar(aes(x = juiz, fill = condenado),position = 'dodge', data = na.omit(basefator), stat="count")

g2=ggplot() + geom_bar(aes(x = sexo, fill = condenado),position = 'dodge', data = na.omit(basefator), stat="count")

g3=ggplot() + geom_bar(aes(x = quantidade_de_reus, fill = condenado),position = 'dodge', data = na.omit(basefator), stat="count")

g4=ggplot() + geom_bar(aes(x = reincidente, fill = condenado),position = 'dodge', data = na.omit(basefator), stat="count")

g5=ggplot() + geom_bar(aes(x = favela, fill = condenado),position = 'dodge', data = na.omit(basefator), stat="count")

g6=ggplot() + geom_bar(aes(x = testemunha_def, fill = condenado),position = 'dodge', data = na.omit(basefator), stat="count")

g7=ggplot() + geom_bar(aes(x = testemunha_acus, fill = condenado),position = 'dodge', data = na.omit(basefator), stat="count")

g8=ggplot() + geom_bar(aes(x = flagrante, fill = condenado),position = 'dodge', data = na.omit(basefator), stat="count")

g9=ggplot() + geom_bar(aes(x = muita_quantidade, fill = condenado),position = 'dodge', data = na.omit(basefator), stat="count")

g10=ggplot() + geom_bar(aes(x = condicao_pessoal, fill = condenado),position = 'dodge', data = na.omit(basefator), stat="count")

g11=ggplot() + geom_bar(aes(x = sumula, fill = condenado),position = 'dodge', data = na.omit(basefator), stat="count")

g12=ggplot() + geom_bar(aes(x = confissao, fill = condenado),position = 'dodge', data = na.omit(basefator), stat="count")

gridExtra::grid.arrange(g1,g2,g3,g4,g5,g6,g7,g8,g9,g10,g11,g12,ncol=2)

```

# Modelos propostos

## Modelo de Regressão linear logístico:

```{r,echo=F}
set.seed(25-10-2017)

#TEste
#Criando indices para treino e teste
amostra=sample(1:2, dim(basefator)[1], replace=T, prob=c(0.9,1-0.9))
train=basefator[amostra==1,]
test=basefator[amostra==2,]
modelo = glm(data=na.omit(train), formula = condenado ~ . , family = binomial("logit"))

```

### Treino 

```{r,echo=F}
library(MASS)
step = stepAIC(modelo, direction = "both")
#Modelo selecionado:
summary(step)
```

### Teste


```{r,echo=F}
predRL = predict(step, newdata = na.omit(test)%>%dplyr::select(c("quantidade_de_reus", "sexo", "favela", "testemunha_acus", "flagrante", "muita_quantidade", "confissao")), type = "response")
a=ifelse(na.omit(test)%>%dplyr::select(condenado)=="Sim",1,0)
b=ifelse(predRL>0.5,1,0)
confusionMatrix(a,b)
```

Modelo ruim!


## Arvores de Decisões (sem cross validation)

Dado os resultados deste modelo, vejamos uma opção alternativa

### Treino

```{r, echo=F}
set.seed(22-10-2017)
amostra=sample(1:2, 2997, replace=T, prob=c(0.632,1-0.632))
train=basefator[amostra==1,]
test=basefator[amostra==2,]

modelo_ad = rpart(data=as.data.frame(train), formula = condenado ~ . , method="class",control = rpart.control(minsplit=1),parms = list(split="gini"))
plot = rpart.plot(modelo_ad, 2, 3)

```

### Teste

```{r, echo=F}
predictions = predict(modelo_ad, test[,-13])
a=ifelse(test%>%dplyr::select(condenado)=="Sim",1,0)
b=ifelse(predictions[,1]>0.5,1,0)
confusionMatrix(a,b)
```


## Arvore de decisoes (com cross validation)

```{r, echo=F}
#k - fold cress validation
trainControl = trainControl(method="cv", number=10, classProbs = T)

#Treina uma arvore de decisao
tree = train(condenado ~. , data = na.omit(basefator), method="rpart", trControl=trainControl)

#O modelo sera:
rattle::fancyRpartPlot(tree$finalModel)

#Verifica o resultado da validacao cruzada
print(tree)
```

### Redes neurais:

#### Treino:

```{r, echo=F}

#Começando a organizar os dados:
base=as.matrix(dados)
X= base[,-13]
Y=decodeClassLabels(base[,13])

#Conjuntos de treinamentos devem ser criados:
d_separado=splitForTrainingAndTest(X,Y,ratio=0.1)

#Comando para normalizar os dados:
d_normalizado=normTrainingAndTestSet(d_separado, dontNormTargets = TRUE, type="0_1")

#Funcao de Treinamento;
#Obs.: quantidade de camadas escondidas e o numero de neuronios foram escolhas arbitrarias
modelo_mlp= mlp(d_separado$inputsTrain,
                d_separado$targetsTrain,
                size=c(3),
                maxit=1000,
                initFunc = "Randomize_Weights",
                #learnFunc = "Std_Backpropagation",
                learnFuncParams = c(0.2),
                hiddenActFunc = "Act_Logistic",
                shufflePatterns = T,
                linOut = T,
                inputsTest = d_separado$inputsTest,
                targetsTest = d_separado$targetsTest);modelo_mlp

#Plotagem para curva de aprendizado:
plot(modelo_mlp$IterativeFitError, type="n", main="Curva de Aprendizagem", xlab="Iteração", ylab="Erro médio quadrado", cex.lab=1.5)
lines(modelo_mlp$IterativeFitError,col="1", lwd=3, cex=2) #Erro quadrado medio a cada iteração para o conjunto de treinamento
lines(modelo_mlp$IterativeTestError, col="2", lwd=3, cex=2) ##Erro quadrado medio a cada iteração para o conjunto de teste
legend("top", c("Treinamento", "Teste"), lty=c(1,1), col=c(1,2)) #Clicar para legenda aparecer

#importando a funcao do Github
library(devtools)
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')

plot.nnet(modelo_mlp)


```

#### Teste

```{r, echo=F}

predictions = predict(modelo_mlp,d_separado$inputsTest)[,1]
a=d_separado$targetsTest[,1]
b=ifelse(predictions>0.5,1,0);length(b)
confusionMatrix(a,b)
```

```{r,eval=F}
library(dplyr)
library(R2OpenBUGS)
library(ggplot2)
library(GGally)
library(coda)
library(lattice)
library(rpart)
library(rpart.plot)

#Testando modelos
getwd()
#Conjunto de dados:
base=readRDS("modelo.rds")
#saveRDS(basefator, "modelo.rds")

dados=na.omit(base)
glimpse(dados)

dados=dados[,-c(4,5,6,12,13,16)]



### Analise Bayesiana usando Openbugs
# Escrevendo o modelo de regressão linear
sink("linreg.txt")
cat("
    model {
    
    # Priori
    
    B0 ~ dnorm(0,0.001)
    B1 ~ dnorm(0,0.001)
    B2 ~ dnorm(0,0.001)
    B3 ~ dnorm(0,0.001)
    B4 ~ dnorm(0,0.001)
    B5 ~ dnorm(0,0.001)
    B6 ~ dnorm(0,0.001)
    B7 ~ dnorm(0,0.001)
    B8 ~ dnorm(0,0.001)
    B9 ~ dnorm(0,0.001)
    B10 ~ dnorm(0,0.001)
    B11 ~ dnorm(0,0.001)
    B12 ~ dnorm(0,0.001)

    # Verossimilhança
    for (i in 1:n) {
    y[i] ~ dbern(p[i])
    logit(p[i]) <- B0+B1*x1[i]+B2*x2[i]+B3*x3[i]+B4*x4[i]+B5*x5[i]+B6*x6[i]+B7*x7[i]+B8*x8[i]+B9*x9[i]+B10*x10[i]+B11*x11[i]+B12*x12[i]
    }
    
    }
    ",fill=TRUE)
sink()


#Conjunto de dados:
glimpse(dados)

# Junte os dados em uma lista
win.data <- list(x1=dados$juiz, x2=dados$quantidade_de_reus, x3=dados$sexo, x4=dados$favela, x5=dados$reincidente, x6=dados$testemunha_def, x7=dados$testemunha_acus, x8=dados$flagrante, x9=dados$muita_quantidade, x10=dados$condicao_pessoal, x11=dados$confissao, x12=dados$sumula,y=dados$condenado, n=length(dados$condenado))

# Função de inicialização
inits <- function(){ list(B0=rnorm(1), B1=rnorm(1),B2=rnorm(1),B3=rnorm(1), B4=rnorm(1),B5=rnorm(1),B6=rnorm(1), B7=rnorm(1),B8=rnorm(1),B9=rnorm(1),B10=rnorm(1), B11=rnorm(1),B12=rnorm(1) )}

# Escolha os parametros que quer estimar
params <- c("B0","B1","B2","B3","B4","B5","B6","B7","B8","B9","B10","B11","B12" )

# Caracteristicas do MCMC
nc = 3 #Numero de cadeias
ni=3000 #Tamanho da cadeira
nb=1000 #Numero de simulação que serão descartadas
#nt=10 #Thinning rate

# Inicie o Amostrador


modelo1.bayes <-bugs(data = win.data, inits = inits, parameters = params,
                     model = "linreg.txt",
                     #n.thin = nt,
                     n.chains = nc,
                     n.burnin = nb,
                     n.iter = ni,debug=F
                        )
print(modelo1.bayes)
plot(modelo1.bayes)


coda.bayes <-bugs(data = win.data, inits = inits, parameters = params,
                     model = "linreg.txt",
                     #n.thin = nt,
                     #n.chains = nc,
                     n.burnin = nb,
                     n.iter = ni,debug=F,codaPkg = T
)

out.coda <- read.bugs(coda.bayes)
library(coda)
xyplot(out.coda)
densityplot(out.coda)
acfplot(out.coda)

gelman.diag(out.coda)
gelman.plot(out.coda)

out.summary <- summary(out.coda,q=c(0.025, 0.975))
out.summary$stat[1:10,"Mean", drop=FALSE]
out.summary$q[1:10, ]
```

