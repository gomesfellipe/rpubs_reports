confusionMatrix(a,b)
basefator=na.omit(basefator)
train=basefator[amostra==1,]
test=basefator[amostra==2,]
trainControl = trainControl(method="cv", number=10, classProbs = T)
tree = train(condenado ~. , data = na.omit(train), method="rpart", trControl=trainControl)
tree = train(condenado ~. , data = na.omit(train), method="rpart", trControl=trainControl)
print(tree)
plot(tree)
predictions = predict(tree, newdata = test[,-19])
a=ifelse(test%>%dplyr::select(condenado)=="Sim",1,0)
b=ifelse(predictions=="Sim",1,0)
b=ifelse(predictions=="Sim",1,0)
confusionMatrix(a,b)
confusionMatrix(a,b)
b
a
set.seed(22-10-2017)
amostra=sample(1:2, 2997, replace=T, prob=c(0.632,1-0.632))
basefator=na.omit(basefator)
train=basefator[amostra==1,]
test=basefator[amostra==2,]
#k - fold cress validation
trainControl = trainControl(method="cv", number=10, classProbs = T)
#Treina uma arvore de decisao
tree = train(condenado ~. , data = na.omit(train), method="rpart", trControl=trainControl)
#Verifica o resultado da validacao cruzada
print(tree)
plot(tree)
predictions = predict(tree, newdata = test[,-19])
a=ifelse(test%>%dplyr::select(condenado)=="Sim",1,0)
b=ifelse(predictions=="Sim",1,0)
confusionMatrix(a,b)
plot(tree$finalModel)
text(tree$finalModel)
plot(tree$finalModel)
text(tree$finalModel)
?train
print(tree)
plot(tree$finalModel)
text(tree$finalModel)
library(rattle)
install.packages(rattle)
install.packages("rattle")
fancyRpartPlot(tree$finalModel)
library(rattle)
library(rattle)
fancyRpartPlot(tree$finalModel)
```
varmodelo%>%
dplyr::select(condenado, everything())%>%
ggpairs( lower = list(continuous = my_fn))
my_fn <- function(data, mapping, method="glm", ...){
p <- ggplot(data = data, mapping = mapping) +
geom_point() +
geom_smooth(method=method, ...)
p
}
varmodelo%>%
dplyr::select(condenado, everything())%>%
ggpairs( lower = list(continuous = my_fn))
na.omit(varmodelo)%>%
dplyr::select(condenado, everything())%>%
ggpairs( lower = list(continuous = my_fn))
base=na.omit(base)
base=as.matrix(base)
base[,-19]
Y=decodeClassLabels(base[,19])
Y
#Plotagem para curva de aprendizado:
plot(modelo_mlp$IterativeFitError, type="n", main="Curva de Aprendizagem", xlab="Iteração", ylab="Erro médio quadrado", cex.lab=1.5)
#Começando a organizar os dados:
base=na.omit(base)
base=as.matrix(base)
X= base[,-19]
Y=decodeClassLabels(base[,19])
#Conjuntos de treinamentos devem ser criados:
d_separado=splitForTrainingAndTest(X,Y,ratio=0.1)
#Comando para normalizar os dados:
d_normalizado=normTrainingAndTestSet(d_separado, dontNormTargets = TRUE, type="0_1")
#Funcao de Treinamento;
#Obs.: quantidade de camadas escondidas e o numero de neuronios foram escolhas arbitrarias
modelo_mlp= mlp(d_separado$inputsTrain,
d_separado$targetsTrain,
size=c(5),
maxit=5000,
initFunc = "Randomize_Weights",
learnFunc = "Std_Backpropagation",
learnFuncParams = c(0.5),
hiddenActFunc = "Act_Logistic",
shufflePatterns = T,
linOut = F,
inputsTest = d_separado$inputsTest,
targetsTest = d_separado$targetsTest);modelo_mlp
#Plotagem para curva de aprendizado:
plot(modelo_mlp$IterativeFitError, type="n", main="Curva de Aprendizagem", xlab="Iteração", ylab="Erro médio quadrado", cex.lab=1.5)
lines(modelo_mlp$IterativeFitError,col="1", lwd=3, cex=2) #Erro quadrado medio a cada iteração para o conjunto de treinamento
lines(modelo_mlp$IterativeTestError, col="2", lwd=3, cex=2) ##Erro quadrado medio a cada iteração para o conjunto de teste
legend("top", c("Treinamento", "Teste"), lty=c(1,1), col=c(1,2)) #Clicar para legenda aparecer
library(devtools)
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')
plot.nnet(modelo_mlp)
predictions = predict(modelo_mlp,d_separado$inputsTest)[,1]
a=d_separado$targetsTest[,1]
b=ifelse(predictions>0.45,0,1);length(b)
confusionMatrix(a,b)
a=d_separado$targetsTest[,2]
b=ifelse(predictions>0.45,0,1);length(b)
confusionMatrix(a,b)
predictions = predict(modelo_mlp,d_separado$inputsTest)[,1]
a=d_separado$targetsTest[,1]
b=ifelse(predictions>0.5,0,1);length(b)
confusionMatrix(a,b)
predict(modelo_mlp,d_separado$inputsTest)[,1]
predict(modelo_mlp,d_separado$inputsTest)
d_separado$inputsTest
modelo_mlp= mlp(d_separado$inputsTrain,
d_separado$targetsTrain,
size=c(5),
maxit=5000,
initFunc = "Randomize_Weights",
learnFunc = "Std_Backpropagation",
learnFuncParams = c(0.2),
hiddenActFunc = "Act_Logistic",
shufflePatterns = T,
linOut = T,
inputsTest = d_separado$inputsTest,
targetsTest = d_separado$targetsTest);modelo_mlp
plot(modelo_mlp$IterativeFitError, type="n", main="Curva de Aprendizagem", xlab="Iteração", ylab="Erro médio quadrado", cex.lab=1.5)
lines(modelo_mlp$IterativeFitError,col="1", lwd=3, cex=2) #Erro quadrado medio a cada iteração para o conjunto de treinamento
lines(modelo_mlp$IterativeTestError, col="2", lwd=3, cex=2) ##Erro quadrado medio a cada iteração para o conjunto de teste
library(devtools)
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')
plot.nnet(modelo_mlp)
predictions = predict(modelo_mlp,d_separado$inputsTest)[,1]
a=d_separado$targetsTest[,1]
b=ifelse(predictions>0.5,0,1);length(b)
confusionMatrix(a,b)
modelo_mlp= mlp(d_separado$inputsTrain,
d_separado$targetsTrain,
size=c(3),
maxit=100000,
initFunc = "Randomize_Weights",
#learnFunc = "Std_Backpropagation",
learnFuncParams = c(0.2),
hiddenActFunc = "Act_Logistic",
shufflePatterns = T,
linOut = T,
inputsTest = d_separado$inputsTest,
targetsTest = d_separado$targetsTest);modelo_mlp
plot(modelo_mlp$IterativeFitError, type="n", main="Curva de Aprendizagem", xlab="Iteração", ylab="Erro médio quadrado", cex.lab=1.5)
plot(modelo_mlp$IterativeFitError, type="n", main="Curva de Aprendizagem", xlab="Iteração", ylab="Erro médio quadrado", cex.lab=1.5)
lines(modelo_mlp$IterativeFitError,col="1", lwd=3, cex=2) #Erro quadrado medio a cada iteração para o conjunto de treinamento
lines(modelo_mlp$IterativeTestError, col="2", lwd=3, cex=2) ##Erro quadrado medio a cada iteração para o conjunto de teste
legend("top", c("Treinamento", "Teste"), lty=c(1,1), col=c(1,2)) #Clicar para legenda aparecer
predictions = predict(modelo_mlp,d_separado$inputsTest)[,1]
a=d_separado$targetsTest[,1]
b=ifelse(predictions>0.5,0,1);length(b)
confusionMatrix(a,b)
set.seed(25-10-2017)
#TEste
modelobay=basefator%>%
dplyr::select(-c(maconha, investigacao, quantidade_de_reus))
modelobay$condenado=relevel(modelobay$condenado, "Sim")
#Criando indices para treino e teste
amostra=sample(1:2, 2997, replace=T, prob=c(0.9,1-0.9))
train=modelobay[amostra==1,]
test=modelobay[amostra==2,]
modelo = glm(data=na.omit(train), formula = condenado ~ . , family = binomial("logit"))
library(MASS)
step = stepAIC(modelo, direction = "top")
library(MASS)
step = stepAIC(modelo, direction = "backward")
#Modelo selecionado:
summary(step)
predRL = predict(step, newdata = na.omit(test)%>%dplyr::select(c("sexo", "cocaina","crack", "favela","testemunha_acus","denuncia", "muita_quantidade","confissao")), type = "response")
a=ifelse(na.omit(test)%>%dplyr::select(condenado)=="Sim",1,0)
b=ifelse(predRL>0.5,1,0)
confusionMatrix(a,b)
predRL
summary(predRL)
boxplot(predRL)
b=ifelse(predRL>0.2,1,0)
b=ifelse(predRL>0.2,1,0)
confusionMatrix(a,b)
b=ifelse(predRL>0.15,1,0)
confusionMatrix(a,b)
b=ifelse(predRL>0.1,1,0)
predRL = predict(step, newdata = na.omit(test)%>%dplyr::select(c("sexo", "cocaina","crack", "favela","testemunha_acus","denuncia", "muita_quantidade","confissao")), type = "response")
a=ifelse(na.omit(test)%>%dplyr::select(condenado)=="Sim",1,0)
b=ifelse(predRL>0.1,1,0)
confusionMatrix(a,b)
predRL = predict(step, newdata = na.omit(test)%>%dplyr::select(c("sexo", "cocaina","crack", "favela","testemunha_acus","denuncia", "muita_quantidade","confissao")), type = "response")
a=ifelse(na.omit(test)%>%dplyr::select(condenado)=="Sim",1,0)
b=ifelse(predRL>0.05,1,0)
confusionMatrix(a,b)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(GGally)
library(gridExtra)
library(ROCR)
library(tabplot)
library(rpart)
library(rpart.plot)
library(knitr)
library(kableExtra)
library(lubridate)
library(RSNNS)
library(caret)
library(FSelector)   #Metodos para selecionar os atributos
library("RSNNS")
library("nnet")
library("devtools")
library("scales")
library("reshape")
library(rattle)
base=readRDS("modelo.rds")
glimpse(base)
data=readRDS("suicidio.rds")
basefator=readRDS("modelofator.rds")
glimpse(basefator)
basefator=readRDS("modelofator.rds")
glimpse(basefator)
glimpse(base)
glimpse(basefator)
basefator=basefator[,-c(4,5,6,12,13,16)]
glimpse(basefator)
base=base[,-c(4,5,6,12,13,16)]
glimpse(basefator)
glimpse(base)
dados=na.omit(base)
glimpse(base)
glimpse(basefator)
basefator$condenado
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(GGally)
library(gridExtra)
library(ROCR)
library(tabplot)
library(rpart)
library(rpart.plot)
library(knitr)
library(kableExtra)
library(lubridate)
library(RSNNS)
library(caret)
library(FSelector)   #Metodos para selecionar os atributos
library("RSNNS")
library("nnet")
library("devtools")
library("scales")
library("reshape")
library(rattle)
#Leitura dos dados
base=readRDS("modelo.rds")
data=readRDS("suicidio.rds")
basefator=readRDS("modelofator.rds")
basefator=basefator[,-c(4,5,6,12,13,16)]
base=base[,-c(4,5,6,12,13,16)]
dados=na.omit(base)
Condenado=c(na.omit(c(data$condenado_art33,data$condenado_art35,data$condenado_art37)))
Artigo=c(rep("33", length(na.omit(data$condenado_art33))),rep("35", length(na.omit(data$condenado_art35))),rep("37", length(na.omit(data$condenado_art37))))
c=data.frame(cbind(Condenado,Artigo))
levels(c$Condenado)=c("Nao", "Sim")
ggplot(data=c,aes( x=Condenado, fill=Artigo))+geom_bar(position="dodge")
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(GGally)
library(gridExtra)
library(ROCR)
library(tabplot)
library(rpart)
library(rpart.plot)
library(knitr)
library(kableExtra)
library(lubridate)
library(RSNNS)
library(caret)
library(FSelector)   #Metodos para selecionar os atributos
library("RSNNS")
library("nnet")
library("devtools")
library("scales")
library("reshape")
library(rattle)
#Leitura dos dados
base=readRDS("modelo.rds")
data=readRDS("suicidio.rds")
basefator=readRDS("modelofator.rds")
basefator=basefator[,-c(4,5,6,12,13,16)]
base=base[,-c(4,5,6,12,13,16)]
dados=na.omit(base)
Condenado=c(na.omit(c(data$condenado_art33,data$condenado_art35,data$condenado_art37)))
Artigo=c(rep("33", length(na.omit(data$condenado_art33))),rep("35", length(na.omit(data$condenado_art35))),rep("37", length(na.omit(data$condenado_art37))))
c=data.frame(cbind(Condenado,Artigo))
levels(c$Condenado)=c("Nao", "Sim")
ggplot(data=c,aes( x=Condenado, fill=Artigo))+geom_bar(position="dodge")
kable(basefator, "html")%>%
kable_styling()%>%
scroll_box(width = "1000px", height = "500px")
tableplot(na.omit(basefator), sortCol = condenado)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(GGally)
library(gridExtra)
library(ROCR)
library(tabplot)
library(rpart)
library(rpart.plot)
library(knitr)
library(kableExtra)
library(lubridate)
library(RSNNS)
library(caret)
library(FSelector)   #Metodos para selecionar os atributos
library("RSNNS")
library("nnet")
library("devtools")
library("scales")
library("reshape")
library(rattle)
#Leitura dos dados
base=readRDS("modelo.rds")
data=readRDS("suicidio.rds")
basefator=readRDS("modelofator.rds")
basefator=basefator[,-c(4,5,6,12,13,16)]
base=base[,-c(4,5,6,12,13,16)]
dados=na.omit(base)
glimpse(dados)
glimpse(basefator)
basefator$condenado=as.factor(basefator$condenado)
basefator$condenado=relevel(basefator$condenado, "Sim")
g1=ggcorr(dados[,],method = c("pairwise", "spearman"), nbreaks = 9)
g2=ggcorr(dados[,],method = c("pairwise", "spearman"), nbreaks = 12)
gridExtra::grid.arrange(g1,g2,ncol=2)
library(dplyr)
library(R2OpenBUGS)
library(ggplot2)
library(GGally)
library(coda)
library(lattice)
library(rpart)
library(rpart.plot)
#Testando modelos
getwd()
#Conjunto de dados:
base=readRDS("modelo.rds")
#saveRDS(basefator, "modelo.rds")
dados=na.omit(base)
glimpse(dados)
dados=dados[,-c(4,5,6,12,13,16)]
### Analise Bayesiana usando Openbugs
# Escrevendo o modelo de regressão linear
sink("linreg.txt")
cat("
model {
# Priori
B0 ~ dnorm(0,0.001)
B1 ~ dnorm(0,0.001)
B2 ~ dnorm(0,0.001)
B3 ~ dnorm(0,0.001)
B4 ~ dnorm(0,0.001)
B5 ~ dnorm(0,0.001)
B6 ~ dnorm(0,0.001)
B7 ~ dnorm(0,0.001)
B8 ~ dnorm(0,0.001)
B9 ~ dnorm(0,0.001)
B10 ~ dnorm(0,0.001)
B11 ~ dnorm(0,0.001)
B12 ~ dnorm(0,0.001)
# Verossimilhança
for (i in 1:n) {
y[i] ~ dbern(p[i])
logit(p[i]) <- B0+B1*x1[i]+B2*x2[i]+B3*x3[i]+B4*x4[i]+B5*x5[i]+B6*x6[i]+B7*x7[i]+B8*x8[i]+B9*x9[i]+B10*x10[i]+B11*x11[i]+B12*x12[i]
}
}
",fill=TRUE)
sink()
#Conjunto de dados:
glimpse(dados)
# Junte os dados em uma lista
win.data <- list(x1=dados$juiz, x2=dados$quantidade_de_reus, x3=dados$sexo, x4=dados$favela, x5=dados$reincidente, x6=dados$testemunha_def, x7=dados$testemunha_acus, x8=dados$flagrante, x9=dados$muita_quantidade, x10=dados$condicao_pessoal, x11=dados$confissao, x12=dados$sumula,y=dados$condenado, n=length(dados$condenado))
# Função de inicialização
inits <- function(){ list(B0=rnorm(1), B1=rnorm(1),B2=rnorm(1),B3=rnorm(1), B4=rnorm(1),B5=rnorm(1),B6=rnorm(1), B7=rnorm(1),B8=rnorm(1),B9=rnorm(1),B10=rnorm(1), B11=rnorm(1),B12=rnorm(1) )}
# Escolha os parametros que quer estimar
params <- c("B0","B1","B2","B3","B4","B5","B6","B7","B8","B9","B10","B11","B12" )
# Caracteristicas do MCMC
nc = 3 #Numero de cadeias
ni=1500 #Tamanho da cadeira
nb=500 #Numero de simulação que serão descartadas
#nt=10 #Thinning rate
# Inicie o Amostrador
modelo1.bayes <-bugs(data = win.data, inits = inits, parameters = params,
model = "linreg.txt",
#n.thin = nt,
n.chains = nc,
n.burnin = nb,
n.iter = ni,debug=F
)
print(modelo1.bayes)
plot(modelo1.bayes)
coda.bayes <-bugs(data = win.data, inits = inits, parameters = params,
model = "linreg.txt",
#n.thin = nt,
#n.chains = nc,
n.burnin = nb,
n.iter = ni,debug=F,codaPkg = T
)
out.coda <- read.bugs(coda.bayes)
library(coda)
xyplot(out.coda)
densityplot(out.coda)
acfplot(out.coda)
gelman.diag(out.coda)
gelman.plot(out.coda)
out.summary <- summary(out.coda,q=c(0.025, 0.975))
out.summary$stat[1:10,"Mean", drop=FALSE]
out.summary$q[1:10, ]
#Razoes de chance
OR1=exp(out.summary$stat[1:10,"Mean", drop=FALSE])
ICbeta1 <- out.summary$q[1:10, ]
ICOR1=exp(ICbeta1)
round((cbind(OR1, ICOR1)),4)
print(modelo1.bayes)
plot(modelo1.bayes)
png("1.png")
dev.off()
setwd("C:/Users/Fellipe/Dropbox/Estatistica Aplicada II/Dados criminais consultoria individual/Apresentacao")
png("1.png")
plot(modelo1.bayes)
dev.off()
dev.off()
png("2.png")
xyplot(out.coda)
dev.off()
densityplot(out.coda)
png("2.png")
xyplot(out.coda)
dev.off()
png("3.png")
densityplot(out.coda)
dev.off()
png("4.png")
acfplot(out.coda)
dev.off()
png("5.png")
gelman.diag(out.coda)
dev.off()
png("6.png")
gelman.plot(out.coda)
dev.off()
print(modelo1.bayes)
out.coda <- read.bugs(coda.bayes)
OR1=exp(out.summary$stat[1:10,"Mean", drop=FALSE])
ICbeta1 <- out.summary$q[1:10, ]
ICOR1=exp(ICbeta1)
round((cbind(OR1, ICOR1)),4)
xyplot(out.coda)
xyplot(out.coda)[1:6]
png("6.png")
gelman.plot(out.coda)
dev.off()
png("5.png")
gelman.diag(out.coda)
dev.off()
out.summary$stat["Mean", drop=FALSE]
out.summary$stat[1:13,"Mean", drop=FALSE]
out.summary$stat[1:14,"Mean", drop=FALSE]
out.summary$stat[1:15,"Mean", drop=FALSE]
out.summary <- summary(out.coda,q=c(0.025, 0.975))
out.summary$stat[1:14,"Mean", drop=FALSE]
out.summary$q[1:14, ]
OR1=exp(out.summary$stat[1:14,"Mean", drop=FALSE])
ICbeta1 <- out.summary$q[1:14, ]
ICOR1=exp(ICbeta1)
round((cbind(OR1, ICOR1)),4)
setwd("C:/Users/Fellipe/Dropbox/Estatistica Aplicada II/Dados criminais consultoria individual/Apresentacao")
setwd("C:/Users/Fellipe/Dropbox/Estatistica Aplicada II/Dados criminais consultoria individual/Apresentacao")
knitr::opts_chunk$set(echo = TRUE)
#Pacotes utilizados:
library(dplyr)
library(R2OpenBUGS)
library(ggplot2)
library(GGally)
library(coda)
library(lattice)
library(rpart)
library(rpart.plot)
library(gridExtra)
library(ROCR)
library(tabplot)
library(knitr)
library(kableExtra)
library(lubridate)
library(RSNNS)
library(caret)
library(FSelector)   #Metodos para selecionar os atributos
library(RSNNS)
library(nnet)
library(devtools)
library(scales)
library(reshape)
library(rattle)
#Dados que serão utilizados:
base=readRDS("modelo.rds")
basefator=readRDS("modelofator.rds")
